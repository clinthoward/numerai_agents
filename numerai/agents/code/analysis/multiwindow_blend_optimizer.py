from __future__ import annotations

import argparse
from pathlib import Path
from typing import Any

import numpy as np
import pandas as pd

from agents.code.metrics import numerai_metrics


AGENTS_DIR = Path(__file__).resolve().parents[2]
NUMERAI_DIR = AGENTS_DIR.parent
REPO_ROOT = NUMERAI_DIR.parent
DEFAULT_EXPERIMENT_DIR = AGENTS_DIR / "experiments" / "arrowstreet_integration"
DEFAULT_BASELINE_RUN = "d02_confirm_ranked128_twostage_w050_full"
# Use a stable specialist checkpoint by default; some e* blend names can be
# regenerated by later frontier runs and drift in lineage over time.
DEFAULT_SPECIALIST_RUN = "f10_full_target_main_orth_beta075"
DEFAULT_COMPARISON_RUNS = [
    "g11_60e03_40d02",
    "g12_50e03_50d02",
    "g13_40e03_60d02",
    "g14_30e03_70d02",
]
DEFAULT_BENCHMARK_MODEL = "v52_lgbm_ender20"


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description=(
            "Constrained multi-window blend optimization for two-run blends "
            "(baseline + specialist)."
        )
    )
    parser.add_argument(
        "--experiment-dir",
        type=Path,
        default=DEFAULT_EXPERIMENT_DIR,
        help="Path to experiment directory containing predictions/.",
    )
    parser.add_argument(
        "--baseline-run",
        type=str,
        default=DEFAULT_BASELINE_RUN,
        help="Baseline run name (prediction parquet stem).",
    )
    parser.add_argument(
        "--specialist-run",
        type=str,
        default=DEFAULT_SPECIALIST_RUN,
        help="Specialist run name to blend with baseline.",
    )
    parser.add_argument(
        "--comparison-runs",
        type=str,
        default=",".join(DEFAULT_COMPARISON_RUNS),
        help="Optional comma-separated runs to compare against optimized blend.",
    )
    parser.add_argument(
        "--benchmark-model",
        type=str,
        default=DEFAULT_BENCHMARK_MODEL,
        help="Benchmark model column used for BMC scoring.",
    )
    parser.add_argument(
        "--benchmark-data-path",
        type=str,
        default="v5.2/full_benchmark_models.parquet",
        help="Path to benchmark models parquet.",
    )
    parser.add_argument(
        "--num-windows",
        type=int,
        default=3,
        help="Number of trailing windows.",
    )
    parser.add_argument(
        "--window-size-eras",
        type=int,
        default=192,
        help="Era count per trailing window.",
    )
    parser.add_argument(
        "--grid-step",
        type=float,
        default=0.02,
        help="Specialist weight step size in [0, 1].",
    )
    parser.add_argument(
        "--corr-floor-delta",
        type=float,
        default=-0.0010,
        help="Primary worst-window corr delta floor vs baseline.",
    )
    parser.add_argument(
        "--corr-hard-fail-delta",
        type=float,
        default=-0.0015,
        help="Secondary worst-window corr delta floor vs baseline.",
    )
    parser.add_argument(
        "--corr-delta-std-max",
        type=float,
        default=0.0010,
        help="Maximum std of corr deltas across windows.",
    )
    parser.add_argument(
        "--output-prefix",
        type=str,
        default="v8_multiwindow_optimizer",
        help="Prefix for output artifacts.",
    )
    parser.add_argument(
        "--save-selected-prediction",
        action=argparse.BooleanOptionalAction,
        default=True,
        help="Save a parquet prediction file for the selected optimized blend.",
    )
    return parser.parse_args()


def _resolve_repo_path(path: str | Path) -> Path:
    candidate = Path(path).expanduser()
    if candidate.is_absolute():
        return candidate.resolve()
    if candidate.parts and candidate.parts[0] in {NUMERAI_DIR.name, AGENTS_DIR.name}:
        return (REPO_ROOT / candidate).resolve()
    return (NUMERAI_DIR / candidate).resolve()


def _sort_eras(eras: pd.Series) -> list[Any]:
    def _key(value: Any) -> tuple[int, str]:
        try:
            return (0, f"{int(value):012d}")
        except (TypeError, ValueError):
            return (1, str(value))

    return sorted(pd.unique(eras), key=_key)


def _rank_by_era(values: pd.Series, eras: pd.Series) -> np.ndarray:
    ranked = values.groupby(eras).rank(method="average", pct=True)
    return ranked.to_numpy(dtype=np.float64, copy=False)


def _evaluate_prediction(
    df: pd.DataFrame,
    pred_col: str,
    benchmark_col: str,
    *,
    target_col: str = "target",
    era_col: str = "era",
) -> dict[str, float]:
    per_corr = numerai_metrics.per_era_corr(df, [pred_col], target_col, era_col)[pred_col]
    per_bmc = numerai_metrics.per_era_bmc(
        df, [pred_col], benchmark_col, target_col, era_col
    )[pred_col]
    corr_summary = numerai_metrics.score_summary(per_corr)
    bmc_summary = numerai_metrics.score_summary(per_bmc)
    bmc_last = numerai_metrics.score_summary(per_bmc.tail(200))
    return {
        "corr_mean": float(corr_summary["mean"]),
        "corr_sharpe": float(corr_summary["sharpe"]),
        "bmc_mean": float(bmc_summary["mean"]),
        "bmc_sharpe": float(bmc_summary["sharpe"]),
        "bmc_last200_mean": float(bmc_last["mean"]),
        "bmc_last200_sharpe": float(bmc_last["sharpe"]),
    }


def _write_markdown_table(df: pd.DataFrame, path: Path) -> None:
    display = df.copy()
    numeric_cols = [
        col for col in display.columns if pd.api.types.is_numeric_dtype(display[col])
    ]
    for col in numeric_cols:
        display[col] = display[col].map(lambda v: f"{v:.6f}")
    cols = list(display.columns)
    header = "| " + " | ".join(cols) + " |"
    separator = "| " + " | ".join(["---"] * len(cols)) + " |"
    rows = [
        "| " + " | ".join(str(row[col]) for col in cols) + " |"
        for _, row in display.iterrows()
    ]
    path.write_text("\n".join([header, separator, *rows]) + "\n", encoding="utf-8")


def _load_runs(
    predictions_dir: Path, runs: list[str]
) -> tuple[pd.DataFrame, list[str]]:
    merged: pd.DataFrame | None = None
    loaded: list[str] = []
    for run in runs:
        path = predictions_dir / f"{run}.parquet"
        if not path.exists():
            continue
        cols = numerai_metrics._parquet_columns(path)  # noqa: SLF001
        required = [col for col in ["id", "era", "target", "prediction"] if col in cols]
        if set(required) != {"id", "era", "target", "prediction"}:
            continue
        frame = pd.read_parquet(path, columns=required).rename(columns={"prediction": run})
        if merged is None:
            merged = frame
        else:
            merged = merged.merge(frame[["id", "era", run]], on=["id", "era"], how="inner")
        loaded.append(run)
    if merged is None or not loaded:
        raise FileNotFoundError("No compatible candidate prediction files found.")
    return merged, loaded


def _build_windows(eras: list[Any], num_windows: int, window_size: int) -> list[list[Any]]:
    windows: list[list[Any]] = []
    n = len(eras)
    for i in range(num_windows):
        end = n - i * window_size
        start = max(0, end - window_size)
        if start >= end:
            break
        window = eras[start:end]
        if not window:
            continue
        windows.append(window)
    return windows


def _alpha_grid(step: float) -> np.ndarray:
    if step <= 0.0:
        return np.array([0.0, 1.0], dtype=np.float64)
    raw = np.arange(0.0, 1.0 + step * 0.5, step, dtype=np.float64)
    grid = np.clip(np.round(raw, 8), 0.0, 1.0)
    if grid[0] != 0.0:
        grid = np.insert(grid, 0, 0.0)
    if grid[-1] != 1.0:
        grid = np.append(grid, 1.0)
    return np.unique(grid)


def _select_best(
    df: pd.DataFrame,
    *,
    corr_floor: float,
    corr_std_max: float,
) -> pd.Series | None:
    eligible = df[
        (df["min_delta_corr"] >= corr_floor)
        & (df["std_delta_corr"] <= corr_std_max)
        & (df["min_delta_bmc200"] > 0.0)
    ].copy()
    if eligible.empty:
        return None
    eligible = eligible.sort_values(
        ["mean_delta_bmc200", "mean_delta_corr"],
        ascending=[False, False],
    )
    return eligible.iloc[0]


def main() -> None:
    args = parse_args()
    experiment_dir = _resolve_repo_path(args.experiment_dir)
    predictions_dir = experiment_dir / "predictions"

    comparison_runs = [item.strip() for item in args.comparison_runs.split(",") if item.strip()]
    run_list = [args.baseline_run, args.specialist_run, *comparison_runs]
    # Stable order and dedupe
    deduped_runs = list(dict.fromkeys(run_list))

    base_df, loaded_runs = _load_runs(predictions_dir, deduped_runs)
    if args.baseline_run not in loaded_runs:
        raise ValueError(f"Baseline run not loaded: {args.baseline_run}")
    if args.specialist_run not in loaded_runs:
        raise ValueError(f"Specialist run not loaded: {args.specialist_run}")
    loaded_comparison = [run for run in comparison_runs if run in loaded_runs]

    benchmark_path = _resolve_repo_path(args.benchmark_data_path)
    benchmark, benchmark_col = numerai_metrics.load_benchmark_predictions_from_path(
        benchmark_path,
        args.benchmark_model,
        era_col="era",
        id_col="id",
    )
    base_df = numerai_metrics.attach_benchmark_predictions(
        base_df, benchmark, benchmark_col, era_col="era", id_col="id"
    )

    eras = _sort_eras(base_df["era"])
    windows = _build_windows(
        eras,
        num_windows=max(1, int(args.num_windows)),
        window_size=max(1, int(args.window_size_eras)),
    )
    if not windows:
        raise ValueError("No windows were generated; adjust num-windows/window-size-eras.")

    window_frames: list[pd.DataFrame] = []
    baseline_by_window: list[dict[str, float]] = []
    for window_eras in windows:
        window_df = base_df[base_df["era"].isin(window_eras)].copy()
        window_frames.append(window_df)
        baseline_by_window.append(
            _evaluate_prediction(window_df, args.baseline_run, benchmark_col)
        )

    grid_rows: list[dict[str, Any]] = []
    for alpha in _alpha_grid(float(args.grid_step)):
        bmc_deltas: list[float] = []
        corr_deltas: list[float] = []
        corr_vals: list[float] = []
        bmc_vals: list[float] = []
        bmc200_vals: list[float] = []
        for w_idx, window_df in enumerate(window_frames):
            baseline_metrics = baseline_by_window[w_idx]
            blend_raw = (
                float(alpha) * window_df[args.specialist_run]
                + (1.0 - float(alpha)) * window_df[args.baseline_run]
            )
            ranked_blend = _rank_by_era(blend_raw, window_df["era"])
            eval_df = window_df[["id", "era", "target", benchmark_col]].copy()
            eval_df["prediction"] = ranked_blend
            blend_metrics = _evaluate_prediction(eval_df, "prediction", benchmark_col)
            bmc_delta = (
                blend_metrics["bmc_last200_mean"] - baseline_metrics["bmc_last200_mean"]
            )
            corr_delta = blend_metrics["corr_mean"] - baseline_metrics["corr_mean"]
            bmc_deltas.append(float(bmc_delta))
            corr_deltas.append(float(corr_delta))
            corr_vals.append(float(blend_metrics["corr_mean"]))
            bmc_vals.append(float(blend_metrics["bmc_mean"]))
            bmc200_vals.append(float(blend_metrics["bmc_last200_mean"]))
        grid_rows.append(
            {
                "alpha_specialist": float(alpha),
                "alpha_baseline": float(1.0 - float(alpha)),
                "mean_corr": float(np.mean(corr_vals)),
                "mean_bmc": float(np.mean(bmc_vals)),
                "mean_bmc200": float(np.mean(bmc200_vals)),
                "mean_delta_corr": float(np.mean(corr_deltas)),
                "mean_delta_bmc200": float(np.mean(bmc_deltas)),
                "min_delta_corr": float(np.min(corr_deltas)),
                "min_delta_bmc200": float(np.min(bmc_deltas)),
                "std_delta_corr": float(np.std(corr_deltas, ddof=0)),
                "positive_bmc200_windows": int(np.sum(np.asarray(bmc_deltas) > 0.0)),
                "total_windows": int(len(bmc_deltas)),
                "positive_bmc200_ratio": float(
                    np.sum(np.asarray(bmc_deltas) > 0.0) / max(1, len(bmc_deltas))
                ),
            }
        )

    grid_df = pd.DataFrame(grid_rows).sort_values("alpha_specialist")
    grid_df = grid_df.reset_index(drop=True)

    best_unconstrained = grid_df.sort_values(
        ["mean_delta_bmc200", "mean_delta_corr"], ascending=[False, False]
    ).iloc[0]
    best_primary = _select_best(
        grid_df,
        corr_floor=float(args.corr_floor_delta),
        corr_std_max=float(args.corr_delta_std_max),
    )
    best_secondary = _select_best(
        grid_df,
        corr_floor=float(args.corr_hard_fail_delta),
        corr_std_max=float(args.corr_delta_std_max),
    )

    selected_rows: list[dict[str, Any]] = []
    selected_rows.append(
        {
            "slot": "best_unconstrained",
            "alpha_specialist": float(best_unconstrained["alpha_specialist"]),
            "alpha_baseline": float(best_unconstrained["alpha_baseline"]),
            "mean_delta_bmc200": float(best_unconstrained["mean_delta_bmc200"]),
            "mean_delta_corr": float(best_unconstrained["mean_delta_corr"]),
            "min_delta_corr": float(best_unconstrained["min_delta_corr"]),
            "std_delta_corr": float(best_unconstrained["std_delta_corr"]),
            "rule": "max_mean_delta_bmc200",
        }
    )
    if best_primary is not None:
        selected_rows.append(
            {
                "slot": "primary_balanced",
                "alpha_specialist": float(best_primary["alpha_specialist"]),
                "alpha_baseline": float(best_primary["alpha_baseline"]),
                "mean_delta_bmc200": float(best_primary["mean_delta_bmc200"]),
                "mean_delta_corr": float(best_primary["mean_delta_corr"]),
                "min_delta_corr": float(best_primary["min_delta_corr"]),
                "std_delta_corr": float(best_primary["std_delta_corr"]),
                "rule": "strict_corr_floor_and_corr_vol",
            }
        )
    if best_secondary is not None:
        is_duplicate = False
        if best_primary is not None:
            is_duplicate = (
                float(best_primary["alpha_specialist"])
                == float(best_secondary["alpha_specialist"])
            )
        if not is_duplicate:
            selected_rows.append(
                {
                    "slot": "secondary_high_bmc",
                    "alpha_specialist": float(best_secondary["alpha_specialist"]),
                    "alpha_baseline": float(best_secondary["alpha_baseline"]),
                    "mean_delta_bmc200": float(best_secondary["mean_delta_bmc200"]),
                    "mean_delta_corr": float(best_secondary["mean_delta_corr"]),
                    "min_delta_corr": float(best_secondary["min_delta_corr"]),
                    "std_delta_corr": float(best_secondary["std_delta_corr"]),
                    "rule": "hard_corr_floor_and_corr_vol",
                }
            )

    selected_df = pd.DataFrame(selected_rows)

    comparison_rows: list[dict[str, Any]] = []
    for run in [args.baseline_run, *loaded_comparison]:
        corr_deltas: list[float] = []
        bmc_deltas: list[float] = []
        corr_vals: list[float] = []
        bmc_vals: list[float] = []
        bmc200_vals: list[float] = []
        for w_idx, window_df in enumerate(window_frames):
            baseline_metrics = baseline_by_window[w_idx]
            metrics = _evaluate_prediction(window_df, run, benchmark_col)
            corr_delta = metrics["corr_mean"] - baseline_metrics["corr_mean"]
            bmc_delta = metrics["bmc_last200_mean"] - baseline_metrics["bmc_last200_mean"]
            corr_deltas.append(float(corr_delta))
            bmc_deltas.append(float(bmc_delta))
            corr_vals.append(float(metrics["corr_mean"]))
            bmc_vals.append(float(metrics["bmc_mean"]))
            bmc200_vals.append(float(metrics["bmc_last200_mean"]))
        comparison_rows.append(
            {
                "run": run,
                "mean_corr": float(np.mean(corr_vals)),
                "mean_bmc": float(np.mean(bmc_vals)),
                "mean_bmc200": float(np.mean(bmc200_vals)),
                "mean_delta_corr": float(np.mean(corr_deltas)),
                "mean_delta_bmc200": float(np.mean(bmc_deltas)),
                "min_delta_corr": float(np.min(corr_deltas)),
                "min_delta_bmc200": float(np.min(bmc_deltas)),
                "std_delta_corr": float(np.std(corr_deltas, ddof=0)),
                "positive_bmc200_windows": int(np.sum(np.asarray(bmc_deltas) > 0.0)),
                "total_windows": int(len(bmc_deltas)),
                "positive_bmc200_ratio": float(
                    np.sum(np.asarray(bmc_deltas) > 0.0) / max(1, len(bmc_deltas))
                ),
            }
        )
    comparison_df = pd.DataFrame(comparison_rows).sort_values(
        "mean_delta_bmc200", ascending=False
    )

    output_grid_csv = experiment_dir / f"{args.output_prefix}_grid_summary.csv"
    output_grid_md = experiment_dir / f"{args.output_prefix}_grid_summary.md"
    output_selected_csv = experiment_dir / f"{args.output_prefix}_selection_table.csv"
    output_selected_md = experiment_dir / f"{args.output_prefix}_selection_table.md"
    output_compare_csv = experiment_dir / f"{args.output_prefix}_comparison_table.csv"
    output_compare_md = experiment_dir / f"{args.output_prefix}_comparison_table.md"

    grid_df.to_csv(output_grid_csv, index=False)
    selected_df.to_csv(output_selected_csv, index=False)
    comparison_df.to_csv(output_compare_csv, index=False)
    _write_markdown_table(grid_df, output_grid_md)
    _write_markdown_table(selected_df, output_selected_md)
    _write_markdown_table(comparison_df, output_compare_md)

    selected_alpha = None
    selected_slot = None
    if best_primary is not None:
        selected_alpha = float(best_primary["alpha_specialist"])
        selected_slot = "primary_balanced"
    elif best_secondary is not None:
        selected_alpha = float(best_secondary["alpha_specialist"])
        selected_slot = "secondary_high_bmc"
    else:
        selected_alpha = float(best_unconstrained["alpha_specialist"])
        selected_slot = "best_unconstrained"

    selected_pred_path: Path | None = None
    if bool(args.save_selected_prediction):
        alpha_tag = int(round(selected_alpha * 100))
        run_name = (
            f"h20_multiwindow_opt_{selected_slot}_"
            f"{args.specialist_run}_w{alpha_tag:03d}"
        )
        selected_pred_path = predictions_dir / f"{run_name}.parquet"
        selected = base_df[["id", "era", "target"]].copy()
        selected_raw = (
            selected_alpha * base_df[args.specialist_run]
            + (1.0 - selected_alpha) * base_df[args.baseline_run]
        )
        selected["prediction"] = _rank_by_era(selected_raw, base_df["era"])
        selected.to_parquet(selected_pred_path, index=False)

    print("Loaded runs:", ",".join(loaded_runs))
    print("Windows:", ",".join([f"w{i}:{len(w)}" for i, w in enumerate(windows)]))
    print("Saved grid summary CSV:", output_grid_csv)
    print("Saved selection CSV:", output_selected_csv)
    print("Saved comparison CSV:", output_compare_csv)
    if selected_pred_path is not None:
        print("Saved selected prediction:", selected_pred_path)
    print("Top grid rows by mean_delta_bmc200:")
    print(
        grid_df.sort_values("mean_delta_bmc200", ascending=False)
        .head(8)[
            [
                "alpha_specialist",
                "alpha_baseline",
                "mean_delta_bmc200",
                "mean_delta_corr",
                "min_delta_corr",
                "std_delta_corr",
                "positive_bmc200_ratio",
            ]
        ]
        .to_string(index=False)
    )
    print("Selection table:")
    print(selected_df.to_string(index=False))
    print("Comparison table:")
    print(comparison_df.to_string(index=False))


if __name__ == "__main__":
    main()
